# mtext("BINOMIAL", side=2, line=5, cex=1.3 )
# grafico(binomial_uni_n100, main = '', xlab = '', ylab = '')
# grafico(binomial_uni_n250, main = '', xlab = 'Distância', ylab = '')
# grafico(binomial_uni_n500, main = '', xlab = '', ylab = '')
# grafico(binomial_uni_n1000, main = '', xlab = '', ylab = '')
#
# dev.off()
#
# #----------------------------------------------------------------
tri_n50
dir()
tri_n50
tri_n1000
poisson_tri_n1000
normal_tri_n1000
binomial_tri_n1000
normal_tri_n500
library(MASS)       # load the MASS package
library(MASS)       # load the MASS package
tbl = table(survey$Smoke, survey$Exer)
tbl                 # the contingency table
chisq.test(tbl)
load("~/msc/3_th_mcglm/4_estudo_simulacao/SIMULA/betas/binomial_tri_n50.RData")
save.image("~/msc/3_th_mcglm/4_estudo_simulacao/SIMULA/betas/binomial_tri_n50.RData")
load("~/msc/3_th_mcglm/4_estudo_simulacao/SIMULA/betas/binomial_tri_n50.RData")
load("~/msc/3_th_mcglm/4_estudo_simulacao/SIMULA/betas/results0.RData")
time_taken_tri2
1.5*60
90/5
sample_size = 50
n_treatment = 4
betas_normal = c(5,0,0,0)
betas_poisson = c(2.3,0,0,0)
betas_binomial = c(0.5,0,0,0)
n_datasets = 500
n_distances = 20
# tratamentos
trat <- gl(n_treatment, sample_size/n_treatment)
# matriz do modelo
X <- model.matrix(~ trat)
invcdfnames <- c("qnorm","qpois", "qbinom")
cor_matrix <- matrix(c(1.0,  0.75, 0.5,
0.75,  1.0, 0.25,
0.5,  0.25, 1.0
),3,3)
# lista para armazenar os conjuntos de dados
datasets <- list()
#----------------------------------------------------------------
# ESTUDO DE SIMULAÇÃO
#----------------------------------------------------------------
# 500 data sets
# 4 tamanhos amostrais: 50, 100, 250, 500 e 1000
# 3 distribuições: normal, poisson, bernoulli (binomial n=1)
#----------------------------------------------------------------
# Betas normal = 5,0,0,0 (cv = 20%)
# Betas poisson = 2.3,0,0,0 (contagens próximas de 10)
# Betas bernoulli = 0.5,0,0,0 (p próximo de 0.6)
#----------------------------------------------------------------
# 20 pontos: faz-se um decrécimo de beta0/20 e distribui esse
# decrécimo nos demais betas.
# Obtem a distância euclideana do vetor original para o modificado.
# Ao final divide o vetor pelo seu sd para independente dos betas
# iniciais as distancias estarem sempre na mesma escala
#----------------------------------------------------------------
# Univariado normal
# Univariado poisson
# Univariado bernouli
# Trivariado normal
# Trivariado poisson
# Trivariado bernouli
# Trivariado normal, poisson, bernouli
#----------------------------------------------------------------
# bibliotecas necessárias
library(mcglm)
library(Matrix)
#----------------------------------------------------------------
# minhas funções
source('~/msc/3_th_mcglm/0_funcoes/functions.R')
#----------------------------------------------------------------
source("~/msc/3_th_mcglm/4_estudo_simulacao/funcoes_simula/simula_uni.R")
source("~/msc/3_th_mcglm/4_estudo_simulacao/funcoes_simula/simula_tri_normal.R")
source("~/msc/3_th_mcglm/4_estudo_simulacao/funcoes_simula/simula_tri_pois_binom.R")
source("~/msc/3_th_mcglm/4_estudo_simulacao/funcoes_simula/simula_tri.R")
#----------------------------------------------------------------
# Parâmetros
n_datasets = 500
n_treatment = 4
n_distances = 20
betas_normal = c(5,0,0,0)
betas_poisson = c(2.3,0,0,0)
betas_binomial = c(0.5,0,0,0)
#----------------------------------------------------------------
sample_size
sample_size = 50
n_treatment
betas_normal
betas_poisson
betas_binomial
n_datasets
n_distances
# tratamentos
trat <- gl(n_treatment, sample_size/n_treatment)
# matriz do modelo
X <- model.matrix(~ trat)
invcdfnames <- c("qnorm","qpois", "qbinom")
cor_matrix <- matrix(c(1.0,  0.75, 0.5,
0.75,  1.0, 0.25,
0.5,  0.25, 1.0
),3,3)
# lista para armazenar os conjuntos de dados
datasets <- list()
for (i in 1:n_datasets+15) {
# Argumentos das marginais
mu <- X%*%betas_normal
lambda <- exp(X%*%betas_poisson)
p <- exp(X%*%betas_binomial)/(1 + exp(X%*%betas_binomial))
paramslists <- list(
m1 = list(mean = mu, sd = 1),
m2 = list(lambda = lambda),
m3 = list(p = p, size = 1)
)
y <- NORTARA::genNORTARA(sample_size,
cor_matrix,
invcdfnames,
paramslists)
datasets[[i]] <- data.frame(y = y,
x = trat)
names(datasets[[i]]) <- c('y1', 'y2', 'y3', 'x')
print(i)
}
n_datasets+15
for (i in 1:n_datasets+15) {
# Argumentos das marginais
mu <- X%*%betas_normal
lambda <- exp(X%*%betas_poisson)
p <- exp(X%*%betas_binomial)/(1 + exp(X%*%betas_binomial))
paramslists <- list(
m1 = list(mean = mu, sd = 1),
m2 = list(lambda = lambda),
m3 = list(p = p, size = 1)
)
y <- NORTARA::genNORTARA(sample_size,
cor_matrix,
invcdfnames,
paramslists)
datasets[[i]] <- data.frame(y = y,
x = trat)
names(datasets[[i]]) <- c('y1', 'y2', 'y3', 'x')
print(i)
}
i
for (i in 1:(n_datasets+15)) {
# Argumentos das marginais
mu <- X%*%betas_normal
lambda <- exp(X%*%betas_poisson)
p <- exp(X%*%betas_binomial)/(1 + exp(X%*%betas_binomial))
paramslists <- list(
m1 = list(mean = mu, sd = 1),
m2 = list(lambda = lambda),
m3 = list(p = p, size = 1)
)
y <- NORTARA::genNORTARA(sample_size,
cor_matrix,
invcdfnames,
paramslists)
datasets[[i]] <- data.frame(y = y,
x = trat)
names(datasets[[i]]) <- c('y1', 'y2', 'y3', 'x')
print(i)
}
form1 = y1~x
form2 = y2~x
form3 = y3~x
link_normal <- "identity"
variance_normal <- "constant"
link_poisson <- "log"
variance_poisson <- "tweedie"
link_binomial <- "logit"
variance_binomial <- "binomialP"
Z0 <- mc_id(datasets[[1]])
models <- list()
sample_size
sample_size < 100
if (sample_size < 100) {
ca <- list(#verbose = T,
tuning = 1,
max_iter = 100,
tol = 0.5)
} else {
ca <- list()
}
ca
for (i in 1:(n_datasets+15)) {
fit <-
mcglm(linear_pred = c(form1, form2, form3),
matrix_pred = list(Z0,Z0,Z0),
link = c(link_normal, link_poisson, link_binomial),
variance = c(variance_normal, variance_poisson, variance_binomial),
data = datasets[[i]],
control_algorithm = ca)
models[[i]] <- fit
print(i)
}
for (i in 378:(n_datasets+15)) {
fit <-
mcglm(linear_pred = c(form1, form2, form3),
matrix_pred = list(Z0,Z0,Z0),
link = c(link_normal, link_poisson, link_binomial),
variance = c(variance_normal, variance_poisson, variance_binomial),
data = datasets[[i]],
control_algorithm = ca)
models[[i]] <- fit
print(i)
}
models[[377]]
dists <- vector() # vetor para armazenar as distancias
dists[1] <- 0 # distancia inicial 0
hyp_betas_normal <- betas_normal # vetor inicial para distribuir os efeitos
hyp_betas_poisson <- betas_poisson # vetor inicial para distribuir os efeitos
hyp_betas_binomial <- betas_binomial # vetor inicial para distribuir os efeitos
hypothesis <- list() # vetor para armazenar as hipoteses
# hipotese inicial
hypothesis[[1]] <- paste(coef(models[[1]], type = 'beta')$Parameters,
'=',
c(betas_normal,
betas_poisson,
betas_binomial))
# obtenção das distâncias e hipóteses a serem testadas
for (i in 2:n_distances) {
hyp_betas_normal[1] <- hyp_betas_normal[1] - (betas_normal[1]/n_distances)
hyp_betas_normal[2:length(betas_normal)] <- hyp_betas_normal[2:length(betas_normal)] + (betas_normal[1]/n_distances)/(n_treatment-1)
hyp_betas_poisson[1] <- hyp_betas_poisson[1] - (betas_poisson[1]/n_distances)
hyp_betas_poisson[2:length(betas_poisson)] <- hyp_betas_poisson[2:length(betas_poisson)] + (betas_poisson[1]/n_distances)/(n_treatment-1)
hyp_betas_binomial[1] <- hyp_betas_binomial[1] - (betas_binomial[1]/n_distances)
hyp_betas_binomial[2:length(betas_binomial)] <- hyp_betas_binomial[2:length(betas_binomial)] + (betas_binomial[1]/n_distances)/(n_treatment-1)
hypothesis[[i]] <- paste(coef(models[[1]], type = 'beta')$Parameters,
'=',
c(hyp_betas_normal,
hyp_betas_poisson,
hyp_betas_binomial))
dists[[i]] <- dist(rbind(betas_normal,
hyp_betas_normal),
method = "euclidean")
}
dists <- dists/sd(dists)
parameters <- data.frame(Parameters = coef(models[[1]])$Parameters,
Type = coef(models[[1]])$Type)
for (i in 1:(n_datasets+15)) {
parameters[,i+2] <- coef(models[[i]])$Estimates
}
vcovs <- list()
parameters <- data.frame(Parameters = coef(models[[1]])$Parameters,
Type = coef(models[[1]])$Type)
for (i in 1:(n_datasets+15)) {
parameters[,i+2] <- coef(models[[i]])$Estimates
}
parameters <- data.frame(Parameters = coef(models[[1]])$Parameters,
Type = coef(models[[1]])$Type)
for (i in 1:376) {
parameters[,i+2] <- coef(models[[i]])$Estimates
}
for (i in 378:(n_datasets+15)) {
parameters[,i+2] <- coef(models[[i]])$Estimates
}
parameters
parameters[,377+2] <- NA
for (i in 378:(n_datasets+15)) {
parameters[,i+2] <- coef(models[[i]])$Estimates
}
for (i in 1:377) {
vcovs[[i]] <- vcov(models[[i]])
}
for (i in 1:376) {
vcovs[[i]] <- vcov(models[[i]])
}
vcovs[[377]] <- NA
for (i in 378:(n_datasets+15)) {
vcovs[[i]] <- vcov(models[[i]])
}
p_test <- matrix(nrow = length(hypothesis),
ncol = length(models))
for (i in 1:length(models)) {
for (j in 1:length(hypothesis)) {
p_test[j,i] <- try(mc_linear_hypothesis(object =  models[[i]],
hypothesis = hypothesis[[j]])$P_valor)
}
}
p_test
# converte resultado para dataframe
p_test <- as.data.frame(p_test)
# caso tenha falhado, converte para NA
for (i in 1:ncol(p_test)) {
p_test[,i] <- as.numeric(p_test[,i])
}
p_tes
p_test
p_test[,377]
# acrescenta info de distancia
p_test$dist <- dists
rej <- ifelse(p_test[,1:(ncol(p_test)-1)] < 0.05, 1, 0)
df_final <- data.frame(dist = p_test$dist,
rej = (rowSums(rej, na.rm = T)/
(ncol(p_test[ , colSums(is.na(p_test)) == 0])-1)*100))
df_final$distribution <- 'normal/poisson/binomial'
df_final$sample_size <- sample_size
df_final$n_datasets <- (ncol(p_test[ , colSums(is.na(p_test)) == 0])-1)
df_final
rej <- ifelse(p_test[,1:(ncol(p_test))] < 0.05, 1, 0)
index_problems <- names(which(colSums(is.na(p_test)) > 0))
index_problems
rej2 <- rej[,!(colnames(rej) %in% index_problems)][,1:n_datasets]
df_final <- data.frame(dist = dists,
rej = ((rowSums(rej2))/ncol(rej2))*100)
df_final$distribution <- 'normal/poisson/binomial'
df_final$sample_size <- sample_size
#df_final$n_datasets <- (ncol(p_test[ , colSums(is.na(p_test)) == 0])-1)
df_final$n_datasets <- ncol(rej2)
df_final
tri_n50 <- list(hypothesis = hypothesis,
parameters = parameters,
vcovs = vcovs,
p_test = p_test,
df_final = df_final)
tri_n50
save.image("~/msc/3_th_mcglm/4_estudo_simulacao/SIMULA/betas/tri_n50.RData")
load("~/msc/3_th_mcglm/4_estudo_simulacao/SIMULA/betas/tri_n50.RData")
tri_n50$hypothesis
tri_n50$p_test
tri_n50$df_final
load("~/msc/3_th_mcglm/4_estudo_simulacao/SIMULA/betas/binomial_tri_n50.RData")
binomial_tri_n50$df_final
load("~/msc/3_th_mcglm/4_estudo_simulacao/SIMULA/betas/results0.RData")
time_taken_uni + time_taken_tri + time_taken_tri2
time_taken_uni
19557.55/60
(19557.55/60)/60
time_taken_uni
time_taken_tri
time_taken_tri2
load("~/msc/3_th_mcglm/4_estudo_simulacao/taus/SERVIDOR/datasets_poisson_50.RData")
datasets[[2]]
datasets[[1]]
cor(datasets[[1]])
cor(datasets[[1]],2)
round(cor(datasets[[1]]),2)
load("~/msc/3_estudo_simulacao/datasets_poisson_1000_2.RData")
#load no rdata do servidor
length(datasets)
print(nrow(datasets[[i]]))
for (i in 1:length(datasets)) {
print(nrow(datasets[[i]]))
}
i
datasets[[i]])
datasets[[i]]
tail(datasets[[4]])
load("~/msc/3_estudo_simulacao/91datasets_poisson_1000.RData")
length(datasets_1)
#load no rdata do servidor
length(datasets)
inicio <- length(datasets_1)
#for de 1 ate o numero de datasets gerado no servidor
for (i in 1:length(datasets)) {
datasets_1[[(inicio+i)]] <- datasets[[i]]
}
length(datasets_1)
save.image("~/msc/3_estudo_simulacao/95datasets_poisson_1000.RData")
load("~/msc/3_estudo_simulacao/95datasets_poisson_1000.RData")
load("~/msc/3_estudo_simulacao/datasets_poisson_1000_2.RData")
length(datasets_1)
length(datasets_1)
#load no rdata do servidor
length(datasets)
inicio <- length(datasets_1)
#for de 1 ate o numero de datasets gerado no servidor
for (i in 1:length(datasets)) {
datasets_1[[(inicio+i)]] <- datasets[[i]]
}
length(datasets_1)
save.image("~/msc/3_estudo_simulacao/111datasets_poisson_1000.RData")
load("~/msc/3_estudo_simulacao/111datasets_poisson_1000.RData")
load("~/msc/3_estudo_simulacao/datasets_poisson_1000_1.RData")
length(datasets_1)
#load no rdata do servidor
length(datasets)
inicio <- length(datasets_1)
#for de 1 ate o numero de datasets gerado no servidor
for (i in 1:length(datasets)) {
datasets_1[[(inicio+i)]] <- datasets[[i]]
}
length(datasets_1)
save.image("~/msc/3_estudo_simulacao/152datasets_poisson_1000.RData")
dir()
load("~/msc/3_estudo_simulacao/432datasets_poisson_1000.RData")
length(datasets_1)
load("~/msc/3_estudo_simulacao/datasets_poisson_1000_1.RData")
#load no rdata do servidor
length(datasets)
inicio <- length(datasets_1)
#for de 1 ate o numero de datasets gerado no servidor
for (i in 1:length(datasets)) {
datasets_1[[(inicio+i)]] <- datasets[[i]]
}
length(datasets_1)
save.image("~/msc/3_estudo_simulacao/462datasets_poisson_1000.RData")
500-462
load("~/msc/3_estudo_simulacao/462datasets_poisson_1000.RData")
load("~/msc/3_estudo_simulacao/datasets_poisson_1000_1.RData")
length(datasets_1)
#load no rdata do servidor
length(datasets)
inicio <- length(datasets_1)
#for de 1 ate o numero de datasets gerado no servidor
for (i in 1:length(datasets)) {
datasets_1[[(inicio+i)]] <- datasets[[i]]
}
length(datasets_1)
save.image("~/msc/3_estudo_simulacao/467datasets_poisson_1000.RData")
load("~/msc/3_estudo_simulacao/467datasets_poisson_1000.RData")
load("~/msc/3_estudo_simulacao/datasets_poisson_1000_1.RData")
length(datasets_1)
#load no rdata do servidor
length(datasets)
inicio <- length(datasets_1)
#for de 1 ate o numero de datasets gerado no servidor
for (i in 1:length(datasets)) {
datasets_1[[(inicio+i)]] <- datasets[[i]]
}
length(datasets_1)
save.image("~/msc/3_estudo_simulacao/472datasets_poisson_1000.RData")
![](roc.png)
266-237
53+38+52
63+31
143-94
238+49
getwd()
setwd("~/mepc_2022")
rmarkdown::render_site()
265-237
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
quest_22$indica_foto <- c(rep('S/ barba', 53), #53
rep('C/ barba', 63), #116
rep('S/ barba', 38), #154
rep('C/ barba', 31), #185
rep('S/ barba', 52), #237
rep('C/ barba', 28))
table(quest_22$indica_foto)
table(quest_22$indica_foto)
rmarkdown::render_site()
rmarkdown::render_site()
length(c(rep('S/ barba', 53), #53
rep('C/ barba', 63), #116
rep('S/ barba', 38), #154
rep('C/ barba', 31), #185
rep('S/ barba', 52), #237
rep('C/ barba', 38)))
nrow(quest_22)
rmarkdown::render_site()
nrow(quest_22)
length(c(rep('S/ barba', 53), #53
rep('C/ barba', 63), #116
rep('S/ barba', 38), #154
rep('C/ barba', 31), #185
rep('S/ barba', 52), #237
rep('C/ barba', 38)) )
rmarkdown::render_site()
path <- "./Questionário-MEPC-2022 (respostas) - Respostas ao formulário 1.csv"
# Leitura dados 2022
quest_22 <- read.csv(path, sep = ',', encoding = 'UTF-8', header = T)
# Nome das colunas
names(quest_22) <-
c('data_hora', 'instituicao', 'setor_ufpr', 'atividade_ies',
'area_conhec', 'fim_grad', 'inst_grad', 'programas_grad',
'inicio_msc', 'final_msc', 'inst_msc', 'artigos', 'bolsa',
'materias_est', 'materias_est_pos', 'software_sn',
'software_list', 'met_est', 'import_met_est',
'expectativa_pos_pg', 'conhec_trans', 'youtube',
'contat_coord', 'contat_colab', 'ensino_medio1', 'ensino_medio2',
'sexo', 'altura', 'peso', 'nascimento', 'emprego', 'residencia',
'irmaos', 'origem', 'transporte', 'tempo_ies', 'app_transporte',
'pet', 'instrumento', 'redes_sociais', 'atv_fisica_reg',
'atv_fisica_n', 'atv_fisica_qual', 'idade_foto', 'kiki_bouba',
'quadrados')
nrow(quest_22)
quest_22$indica_foto <- c(rep('S/ barba', 53), #53
rep('C/ barba', 63), #116
rep('S/ barba', 38), #154
rep('C/ barba', 31), #185
rep('S/ barba', 52), #237
rep('C/ barba', 38))
quest_22$indica_foto <- c(rep('S/ barba', 53), #53
rep('C/ barba', 63), #116
rep('S/ barba', 38), #154
rep('C/ barba', 31), #185
rep('S/ barba', 52), #237
rep('C/ barba', 40))
quest_22$indica_foto <- c(rep('S/ barba', 53), #53
rep('C/ barba', 63), #116
rep('S/ barba', 38), #154
rep('C/ barba', 31), #185
rep('S/ barba', 52), #237
rep('C/ barba', 70))
quest_22$indica_foto <- c(rep('S/ barba', 53), #53
rep('C/ barba', 63), #116
rep('S/ barba', 38), #154
rep('C/ barba', 31), #185
rep('S/ barba', 52), #237
rep('C/ barba', 60))
table(quest_22$indica_foto)
rmarkdown::render_site()
rmarkdown::render_site()
